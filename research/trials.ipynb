{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e0da72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70c87143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c159f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is config.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "# MODEL_NAME = os.getenv(\"MODEL_NAME\", \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd451660",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e3887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompts.py\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "GREETING_PROMPT = PromptTemplate.from_template(\n",
    "    \"Welcome to TalentScout! I am your Hiring Assistant. \"\n",
    "    \"Before we proceed, please review our privacy policy and consent to data collection. \"\n",
    "    \"Type 'I consent' to continue.\"\n",
    ")\n",
    "\n",
    "INFO_COLLECTION_PROMPT = PromptTemplate.from_template(\n",
    "    \"Let's get started. Please provide the following details:\\n\"\n",
    "    \"Full Name:\\nEmail Address:\\nPhone Number:\\nYears of Experience:\\n\"\n",
    "    \"Desired Position(s):\\nCurrent Location:\"\n",
    ")\n",
    "\n",
    "TECH_STACK_PROMPT = PromptTemplate.from_template(\n",
    "    \"Please list the programming languages, frameworks, databases, and tools you are proficient in.\"\n",
    ")\n",
    "\n",
    "TECH_QUESTION_PROMPT = PromptTemplate.from_template(\n",
    "    \"Generate {num_questions} technical interview questions for a candidate skilled in {tech_stack}. \"\n",
    "    \"Questions should assess both practical and conceptual knowledge.\"\n",
    ")\n",
    "\n",
    "THANK_YOU_PROMPT = PromptTemplate.from_template(\n",
    "    \"Thank you for your responses. We will review your application and contact you about next steps. Goodbye!\"\n",
    ")\n",
    "\n",
    "FALLBACK_PROMPT = PromptTemplate.from_template(\n",
    "    \"I'm sorry, I didn't understand that. Could you please rephrase or provide more details?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f872bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8c367af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm.invoke(\"who are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f44a97d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 38, 'total_tokens': 61, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'cached_tokens': 0}, 'model_name': 'meta-llama/Llama-3.3-70B-Instruct-Turbo-Free', 'system_fingerprint': None, 'id': 'nxHkDQh-4Yz4kd-94c7df275c145a24', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--e5f58946-c133-4b23-bb2c-eb55e90726c6-0', usage_metadata={'input_tokens': 38, 'output_tokens': 23, 'total_tokens': 61, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "75aafaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cryptography in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (45.0.3)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from cryptography) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from cffi>=1.14->cryptography) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install cryptography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1061427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_handler.py\n",
    "import os\n",
    "import json\n",
    "from typing import Dict,Any\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "FERNET_KEY = os.environ.get(\"FERNET_KEY\")\n",
    "if FERNET_KEY is None:\n",
    "    FERNET_KEY = Fernet.generate_key().decode()\n",
    "fernet = Fernet(FERNET_KEY.encode())\n",
    "\n",
    "def encrypt_data(data: Dict[str,Any]) -> bytes:\n",
    "    json_data = json.dumps(data).encode()\n",
    "    return fernet.encrypt(json_data)\n",
    "\n",
    "def decrypt_data(token: bytes)-> dict:\n",
    "    return json.loads(fernet.decrypt(token).decode())\n",
    "\n",
    "def save_candidate_data(candidate_data: Dict[str,Any], filename:str = \"candidate_data.enc\") -> None:\n",
    "    encrypted = encrypt_data(candidate_data)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(encrypted)\n",
    "\n",
    "def load_candidate_data(filename:str = \"candidate_data.enc\") -> Dict[str,Any]:\n",
    "    with open(filename,\"rb\") as f:\n",
    "        encrypted = f.read()\n",
    "    return decrypt_data(encrypted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "24329404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (0.4.8)\n",
      "Requirement already satisfied: langchain-core>=0.1 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langgraph) (0.3.64)\n",
      "Requirement already satisfied: langgraph-checkpoint>=2.0.26 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langgraph) (2.0.26)\n",
      "Requirement already satisfied: langgraph-prebuilt>=0.2.0 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langgraph) (0.2.2)\n",
      "Requirement already satisfied: langgraph-sdk>=0.1.42 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langgraph) (0.1.70)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langgraph) (2.11.5)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.3.45 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langchain-core>=0.1->langgraph) (0.3.45)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langchain-core>=0.1->langgraph) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langchain-core>=0.1->langgraph) (4.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (3.10.18)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (2.4.0)\n",
      "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from langgraph-checkpoint>=2.0.26->langgraph) (1.10.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.3.45->langchain-core>=0.1->langgraph) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "688d2a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#state.py\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import TypedDict, List,Annotated\n",
    "\n",
    "class CandidateInfo(TypedDict):\n",
    "    name: str\n",
    "    email: str\n",
    "    phone: str\n",
    "    years_experience: str\n",
    "    desired_position: str\n",
    "    location: str\n",
    "\n",
    "class ChatbotState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    candidate_info: CandidateInfo\n",
    "    tech_stack: List[str]\n",
    "    questions: List[str]\n",
    "    current_question: int\n",
    "    answers: List[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af854924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Dict, Any\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6d91e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #graph.py\n",
    "from typing import Union, Dict, Any\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# #Node Functions\n",
    "# def greet_and_consent(state: ChatbotState):\n",
    "#     last_msg = state[\"messages\"][-1]\n",
    "#     user_input = last_msg.content  # Get the actual text\n",
    "\n",
    "#     if \"consent\" in user_input.lower():\n",
    "#         return \"collect_info\", {}\n",
    "    \n",
    "#     return {\"messages\": GREETING_PROMPT}\n",
    "\n",
    "# def collect_candidate_info(state: ChatbotState):\n",
    "#     user_input = state[\"messages\"][-1].content\n",
    "#     info = [line.strip() for line in user_input.split('\\n') if line.strip()]\n",
    "#     if len(info) >= 6:\n",
    "#         state[\"candidate_info\"] = {\n",
    "#             \"name\": info[0], \"email\": info[1], \"phone\": info[2],\n",
    "#             \"years_experience\": info[3], \"desired_position\": info[4],\n",
    "#             \"location\": info[5]\n",
    "#         }\n",
    "#     return {\"messages\": INFO_COLLECTION_PROMPT}\n",
    "\n",
    "\n",
    "# def collect_tech_stack(state: ChatbotState):\n",
    "#     user_input = state[\"messages\"][-1].content\n",
    "#     tech_stack = [t.strip() for t in user_input.split(',') if t.strip()]\n",
    "#     if tech_stack:\n",
    "#         state[\"tech_stack\"] = tech_stack\n",
    "#     return {\"messages\": TECH_QUESTION_PROMPT}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def generate_tech_questions_node(state: ChatbotState):\n",
    "#     questions = generate_tech_questions(state[\"tech_stack\"])\n",
    "#     state[\"questions\"] = questions\n",
    "#     state[\"current_question\"] = 0\n",
    "#     return {\"messages\": questions[0] if questions else \"No questions generated.\"}\n",
    "\n",
    "# def handle_qa(state: ChatbotState) -> Union[str, Dict[str, Any]]:\n",
    "#     user_input = state[\"messages\"][-1].content\n",
    "#     answers = state.get(\"answers\", []) + [user_input]\n",
    "#     state[\"answers\"] = answers\n",
    "#     state[\"current_question\"] += 1\n",
    "#     if state[\"current_question\"] < len(state[\"questions\"]):\n",
    "#         msg = state[\"questions\"][state[\"current_question\"]]\n",
    "#         return {\"messages\": msg}\n",
    "#     return \"end\"\n",
    "\n",
    "\n",
    "# def fallback_response(state: ChatbotState):\n",
    "#     return {\"messages\": FALLBACK_PROMPT}\n",
    "\n",
    "# def end_conversation(state: ChatbotState):\n",
    "#     return {\"messages\": THANK_YOU_PROMPT}\n",
    "\n",
    "# #Build Workflow\n",
    "# workflow = StateGraph(state_schema=ChatbotState)\n",
    "# workflow.add_node(\"greet_and_consent\",greet_and_consent)\n",
    "# workflow.add_node(\"collect_info\",collect_candidate_info)\n",
    "# workflow.add_node(\"collect_tech_stack\",collect_tech_stack)\n",
    "# workflow.add_node(\"generate_questions\",generate_tech_questions_node)\n",
    "# workflow.add_node(\"handle_answers\",handle_qa)\n",
    "# workflow.add_node(\"fallback\",fallback_response)\n",
    "# workflow.add_node(\"end\",end_conversation)\n",
    "\n",
    "\n",
    "# workflow.add_edge(\"greet_and_consent\",\"collect_info\")\n",
    "# workflow.add_edge(\"collect_info\",\"collect_tech_stack\")\n",
    "# workflow.add_edge(\"collect_tech_stack\",\"generate_questions\")\n",
    "# workflow.add_edge(\"generate_questions\",\"handle_answers\")\n",
    "# workflow.add_edge(\"handle_answers\",\"end\")\n",
    "# workflow.add_edge(\"fallback\",\"end\")\n",
    "# workflow.set_entry_point(\"greet_and_consent\")\n",
    "\n",
    "# compiled_workflow = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "288be8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tech_questions.py\n",
    "from langchain_together import ChatTogether\n",
    "\n",
    "llm = ChatTogether(model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\")\n",
    "\n",
    "# def generate_tech_questions(tech_stack,num_questions = 3):\n",
    "#     prompt = TECH_QUESTION_PROMPT.format(num_questions=num_questions,tech_stack = \", \".join(tech_stack))\n",
    "#     questions = llm(prompt)\n",
    "#     if isinstance(questions, str):\n",
    "#         return [q.strip() for q in questions.split('\\n') if q.strip()]\n",
    "#     return questions\n",
    "\n",
    "# def generate_tech_questions(tech_stack, num_questions=3):\n",
    "#     prompt = TECH_QUESTION_PROMPT.format(num_questions=num_questions, tech_stack=\", \".join(tech_stack))\n",
    "#     questions = llm(prompt)\n",
    "#     print(f\"These are questions {questions}\")\n",
    "#     # Ensure questions is a list of strings\n",
    "#     if isinstance(questions, str):\n",
    "#         # Split by newline and remove empty lines\n",
    "#         return [q.strip() for q in questions.split('\\n') if q.strip()]\n",
    "#     return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96fd5beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"How would you implement a queue using a linked list in Python\", \"Write a Java program to find the maximum and minimum values in a given array\", \"Explain how you would optimize the performance of a Python function that is currently using excessive memory\"]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "TECH_QUESTION_PROMPT = (\n",
    "    \"Generate {num_questions} technical interview questions for a candidate skilled in {tech_stack}. \"\n",
    "    \"Return ONLY a valid Python list of strings, with each question as a separate string in the list. \"\n",
    "    \"Do not include any explanations, formatting, or code blocksâ€”just the Python list.\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_tech_questions(tech_stack, num_questions=3):\n",
    "    prompt = TECH_QUESTION_PROMPT.format(num_questions=num_questions, tech_stack=\", \".join(tech_stack))\n",
    "    # Wrap prompt in a HumanMessage and pass as a list\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    # The model's response is a message object; get its .content\n",
    "    questions = response.content\n",
    "    return questions\n",
    "\n",
    "# Example usage:\n",
    "question_list = generate_tech_questions(['Python', 'java'])\n",
    "print(question_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc8cf8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e53d378e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing LLM output: malformed node or string: []\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    question_list = ast.literal_eval(question_list)\n",
    "except Exception as e:\n",
    "    print(f\"Error parsing LLM output: {e}\")\n",
    "    question_list = []\n",
    "\n",
    "print(question_list)  # This will now print the first question, not '['"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3f2faea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How would you implement a queue using a linked list in Python\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# This could be a string (from LLM) or a list (already parsed)\n",
    "llm_output = question_list  # This might be a string or a list\n",
    "\n",
    "# Robust parsing\n",
    "if isinstance(llm_output, str):\n",
    "    try:\n",
    "        question_list = ast.literal_eval(llm_output)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing LLM output: {e}\")\n",
    "        question_list = []\n",
    "elif isinstance(llm_output, list):\n",
    "    question_list = llm_output\n",
    "else:\n",
    "    question_list = []\n",
    "\n",
    "# Now you can safely use question_list[0]\n",
    "if question_list:\n",
    "    print(question_list[0])\n",
    "else:\n",
    "    print(\"No questions found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c5890dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How would you implement a thread-safe queue in Python to handle concurrent access from multiple threads\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "llm_output = '[\"How would you implement a thread-safe queue in Python to handle concurrent access from multiple threads\", \"Write a Java program to find the middle element of a singly linked list\", \"Explain how you can use Python\\'s decorators to implement aspect-oriented programming and provide an example of logging function calls\"]'\n",
    "\n",
    "try:\n",
    "    question_list = ast.literal_eval(llm_output)\n",
    "except Exception as e:\n",
    "    print(f\"Error parsing LLM output: {e}\")\n",
    "    question_list = []\n",
    "\n",
    "print(question_list[0])  # This will now print the first question, not '['"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "117ce0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'['"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d790b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = generate_tech_questions(['Python', 'java'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1837a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "042e7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5db6656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_PROMPT = \"Welcome to TalentScout! I am your Hiring Assistant. Before we proceed, please review our privacy policy and consent to data collection. Type 'I consent' to continue.\"\n",
    "INFO_COLLECTION_PROMPT = \"Let's get started. Please provide the following details:\\nFull Name:\\nEmail Address:\\nPhone Number:\\nYears of Experience:\\nDesired Position(s):\\nCurrent Location:\"\n",
    "TECH_STACK_PROMPT = \"Please list the programming languages, frameworks, databases, and tools you are proficient in.\"\n",
    "FALLBACK_PROMPT = \"I'm sorry, I didn't understand that. Could you please rephrase or provide more details?\"\n",
    "THANK_YOU_PROMPT = \"Thank you for your responses. We will review your application and contact you about next steps. Goodbye!\"\n",
    "\n",
    "class CandidateInfo(TypedDict):\n",
    "    name: str\n",
    "    email: str\n",
    "    phone: str\n",
    "    years_experience: str\n",
    "    desired_position: str\n",
    "    location: str\n",
    "\n",
    "class ChatbotState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    candidate_info: CandidateInfo\n",
    "    tech_stack: List[str]\n",
    "    questions: List[str]\n",
    "    current_question: int\n",
    "    answers: List[str]\n",
    "\n",
    "\n",
    "def greet_and_consent(state):\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    print(f\"{state['messages']}\")\n",
    "    user_input = input(\"You: \")\n",
    "    # user_input = last_msg.content\n",
    "    if \"consent\" in user_input.lower():\n",
    "        print(\"consent\")\n",
    "        return {**state, \"next\": \"collect_info\"}\n",
    "    new_msg = AIMessage(content=GREETING_PROMPT)\n",
    "    print(\"after consent\")\n",
    "    return {**state, \"messages\": state[\"messages\"] + [new_msg]}\n",
    "\n",
    "def collect_candidate_info(state):\n",
    "    print(\"collect _candidate stated\")\n",
    "    print(\"Provide user you information like name ,email, phone, years of experince desired_position, location each in new line\")\n",
    "    name = input('Enter your name: ')\n",
    "    email = input('Enter your email: ')\n",
    "    phone = input('Enter your phone number: ')\n",
    "    years_experience = input('Enter your years of experience: ')\n",
    "    desired_position = input('Enter your desired position: ')\n",
    "    location = input('Enter your current location: ')\n",
    "    # Build the candidate_info dict\n",
    "    candidate_info = {\n",
    "        \"name\": name,\n",
    "        \"email\": email,\n",
    "        \"phone\": phone,\n",
    "        \"years_experience\": years_experience,\n",
    "        \"desired_position\": desired_position,\n",
    "        \"location\": location\n",
    "    }\n",
    "\n",
    "    # Update your state\n",
    "    state[\"candidate_info\"] = candidate_info\n",
    "    # print(\"content accessed\")\n",
    "    # info = [line.strip() for line in user_input.split('\\n') if line.strip()]\n",
    "    # if len(info) >= 6:\n",
    "    #     candidate_info = {\n",
    "    #         \"name\": info[0], \"email\": info[1], \"phone\": info[2],\n",
    "    #         \"years_experience\": info[3], \"desired_position\": info[4],\n",
    "    #         \"location\": info[5]\n",
    "    #     }\n",
    "    return {**state, \"candidate_info\": candidate_info}\n",
    "    # new_msg = AIMessage(content=INFO_COLLECTION_PROMPT)\n",
    "    # return {**state, \"messages\": state[\"messages\"] + [new_msg], \"next\": \"collect_info\"}\n",
    "\n",
    "# def collect_tech_stack(state):\n",
    "#     print(\"Collect techs tack started\")\n",
    "#     user_input = input(\"You : \")\n",
    "    \n",
    "#     # last_msg = state[\"messages\"][-1]\n",
    "#     # user_input = last_msg.content\n",
    "#     tech_stack = [t.strip() for t in user_input.split(',') if t.strip()]\n",
    "#     if tech_stack:\n",
    "#         return {**state, \"tech_stack\": tech_stack, \"next\": \"generate_questions\"}\n",
    "#     new_msg = AIMessage(content=TECH_STACK_PROMPT)\n",
    "#     return {**state, \"messages\": state[\"messages\"] + [new_msg]}\n",
    "def collect_tech_stack(state):\n",
    "    print(\"Collect tech stack started\")\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.strip().lower() == \"exit\":\n",
    "        return {**state, \"exit\": True, \"next\": \"end\"}\n",
    "    tech_stack = [t.strip() for t in user_input.split(',') if t.strip()]\n",
    "    if not tech_stack:\n",
    "        print(\"Error: No tech stack provided.\")\n",
    "        return {**state, \"error\": True, \"next\": \"collect_tech_stack\"}\n",
    "    return {**state, \"tech_stack\": tech_stack, \"next\": \"generate_questions\"}\n",
    "\n",
    "\n",
    "def generate_tech_questions_node(state):\n",
    "    print(\"generete tech questions started\")\n",
    "    print(f\"{state.get('tech_stack')}\")\n",
    "    questions = generate_tech_questions(state.get(\"tech_stack\", []))\n",
    "    # print(questions)\n",
    "    first_question = questions[0] if questions else \"No questions generated.\"\n",
    "    new_msg = AIMessage(content=first_question)\n",
    "    return {\n",
    "        **state,\n",
    "        \"questions\": questions,\n",
    "        \"current_question\": 0,\n",
    "        \"messages\": state[\"messages\"] + [new_msg],\n",
    "        \"next\": \"handle_answers\"\n",
    "    }\n",
    "\n",
    "def ask_questions_and_collect_answers(state):\n",
    "    \"\"\"\n",
    "    Loops through each technical question, displays it, gets user input,\n",
    "    and stores all answers in the state[\"answers\"] list.\n",
    "    \"\"\"\n",
    "    questions = state.get(\"questions\", [])\n",
    "    print(f\"these are your questions {questions}\")\n",
    "    if not questions:\n",
    "        print(\"No questions to ask.\")\n",
    "        return state\n",
    "    \n",
    "    if isinstance(questions, str):\n",
    "        try:\n",
    "            question_list = ast.literal_eval(questions)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing LLM output: {e}\")\n",
    "            question_list = []\n",
    "    elif isinstance(questions, list):\n",
    "        question_list = questions\n",
    "    else:\n",
    "        question_list = []\n",
    "\n",
    "    # Now you can safely use question_list[0]\n",
    "    if question_list:\n",
    "        answers = []\n",
    "        for idx, question in enumerate(question_list):\n",
    "            print(f\"Question {idx}: {question}\")\n",
    "            user_answer = input(\"Your answer: \")\n",
    "            answers.append(user_answer)\n",
    "            # Optionally, store the Q&A as a message for conversation history\n",
    "            state[\"messages\"].append(AIMessage(content=question))\n",
    "            state[\"messages\"].append(HumanMessage(content=user_answer))\n",
    "    else:\n",
    "        print(\"No questions found.\")\n",
    "\n",
    "    # answers = []\n",
    "    # for idx, question in enumerate(questions):\n",
    "    #     print(f\"Question {idx}: {question}\")\n",
    "    #     user_answer = input(\"Your answer: \")\n",
    "    #     answers.append(user_answer)\n",
    "    #     # Optionally, store the Q&A as a message for conversation history\n",
    "    #     state[\"messages\"].append(AIMessage(content=question))\n",
    "    #     state[\"messages\"].append(HumanMessage(content=user_answer))\n",
    "\n",
    "    # state[\"answers\"] = answers\n",
    "    state[\"current_question\"] = len(questions)  # All questions answered\n",
    "    return state\n",
    "\n",
    "\n",
    "def fallback_response(state):\n",
    "    new_msg = AIMessage(content=FALLBACK_PROMPT)\n",
    "    print(FALLBACK_PROMPT)\n",
    "    return {**state, \"messages\": state[\"messages\"] + [new_msg], \"next\": \"fallback\"}\n",
    "\n",
    "def end_conversation(state):\n",
    "    new_msg = AIMessage(content=THANK_YOU_PROMPT)\n",
    "    print(\"=== Raw State ===\")\n",
    "    pprint(state)\n",
    "    return {**state, \"messages\": state[\"messages\"] + [new_msg], \"next\": \"end\"}\n",
    "\n",
    "\n",
    "#Build Workflow\n",
    "workflow = StateGraph(state_schema=ChatbotState)\n",
    "workflow.add_node(\"greet_and_consent\",greet_and_consent)\n",
    "workflow.add_node(\"collect_info\",collect_candidate_info)\n",
    "workflow.add_node(\"collect_tech_stack\",collect_tech_stack)\n",
    "workflow.add_node(\"generate_questions\",generate_tech_questions_node)\n",
    "workflow.add_node(\"handle_answers\",ask_questions_and_collect_answers)\n",
    "workflow.add_node(\"fallback\",fallback_response)\n",
    "workflow.add_node(\"end\",end_conversation)\n",
    "\n",
    "\n",
    "workflow.add_edge(\"greet_and_consent\",\"collect_info\")\n",
    "workflow.add_edge(\"collect_info\",\"collect_tech_stack\")\n",
    "workflow.add_edge(\"collect_tech_stack\",\"generate_questions\")\n",
    "workflow.add_edge(\"generate_questions\",\"handle_answers\")\n",
    "workflow.add_edge(\"handle_answers\",\"fallback\")\n",
    "workflow.add_edge(\"fallback\",\"end\")\n",
    "workflow.set_entry_point(\"greet_and_consent\")\n",
    "\n",
    "compiled_workflow = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d116087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = {\"role\": \"user\", \"content\": \"Hi\"}\n",
    "initial_state = {\n",
    "    \"messages\": [input_message],\n",
    "    \"answers\": [],\n",
    "    \"current_question\": 0,\n",
    "    \"questions\": [],\n",
    "    \"tech_stack\": [],\n",
    "    \"candidate_info\": {\n",
    "        \"name\": \"\", \"email\": \"\", \"phone\": \"\",\n",
    "        \"years_experience\": \"\", \"desired_position\": \"\", \"location\": \"\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "648c26f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}, id='30212a3b-7487-4592-8e31-db9a50e7fe41')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consent\n",
      "collect _candidate stated\n",
      "Provide user you information like name ,email, phone, years of experince desired_position, location each in new line\n",
      "Collect tech stack started\n",
      "generete tech questions started\n",
      "[]\n",
      "these are your questions [\"Explain how you would implement a singleton class in Python, ensuring thread safety and lazy initialization\", \"Write a Python function to find the first duplicate in an array of integers, optimizing for time complexity\", \"Describe a strategy for optimizing the performance of a Python application that relies heavily on file I/O operations, including caching and buffering techniques\"]\n",
      "Question 0: Explain how you would implement a singleton class in Python, ensuring thread safety and lazy initialization\n",
      "Question 1: Write a Python function to find the first duplicate in an array of integers, optimizing for time complexity\n",
      "Question 2: Describe a strategy for optimizing the performance of a Python application that relies heavily on file I/O operations, including caching and buffering techniques\n",
      "I'm sorry, I didn't understand that. Could you please rephrase or provide more details?\n",
      "=== Raw State ===\n",
      "{'answers': [],\n",
      " 'candidate_info': {'desired_position': 'df',\n",
      "                    'email': 'df',\n",
      "                    'location': 'df',\n",
      "                    'name': 'dk',\n",
      "                    'phone': 'df',\n",
      "                    'years_experience': 'df'},\n",
      " 'current_question': 387,\n",
      " 'messages': [HumanMessage(content='Hi', additional_kwargs={}, response_metadata={}, id='30212a3b-7487-4592-8e31-db9a50e7fe41'),\n",
      "              AIMessage(content='[', additional_kwargs={}, response_metadata={}, id='b5d0a654-a2d7-4431-810c-d474848fec61'),\n",
      "              AIMessage(content='Explain how you would implement a singleton class in Python, ensuring thread safety and lazy initialization', additional_kwargs={}, response_metadata={}, id='3fea6a09-c7bd-4e3e-a5d3-57345f20b693'),\n",
      "              HumanMessage(content='ds', additional_kwargs={}, response_metadata={}, id='126f3a88-4e11-4dc0-80ee-4204e84021e6'),\n",
      "              AIMessage(content='Write a Python function to find the first duplicate in an array of integers, optimizing for time complexity', additional_kwargs={}, response_metadata={}, id='0d78f7c3-3f34-4e96-827c-5b8e59cfa0ca'),\n",
      "              HumanMessage(content='ds', additional_kwargs={}, response_metadata={}, id='dc8d4298-2dc3-4db0-be93-f5dd7e438777'),\n",
      "              AIMessage(content='Describe a strategy for optimizing the performance of a Python application that relies heavily on file I/O operations, including caching and buffering techniques', additional_kwargs={}, response_metadata={}, id='cde36644-68c8-432f-863e-0db11941524c'),\n",
      "              HumanMessage(content='sd', additional_kwargs={}, response_metadata={}, id='35656d48-e68c-47e5-ae4a-03198ad5a7c2'),\n",
      "              AIMessage(content=\"I'm sorry, I didn't understand that. Could you please rephrase or provide more details?\", additional_kwargs={}, response_metadata={}, id='9bd15d80-66b6-4c9c-9321-a03ab6b7596e')],\n",
      " 'questions': '[\"Explain how you would implement a singleton class in Python, '\n",
      "              'ensuring thread safety and lazy initialization\", \"Write a '\n",
      "              'Python function to find the first duplicate in an array of '\n",
      "              'integers, optimizing for time complexity\", \"Describe a strategy '\n",
      "              'for optimizing the performance of a Python application that '\n",
      "              'relies heavily on file I/O operations, including caching and '\n",
      "              'buffering techniques\"]',\n",
      " 'tech_stack': []}\n"
     ]
    }
   ],
   "source": [
    "result = compiled_workflow.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b304ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d0b0732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Hi I am here for interview', additional_kwargs={}, response_metadata={}, id='d433f064-bc46-4c7f-88d8-fadb4bdc7cd4')]\n",
      "consent\n",
      "collect _candidate stated\n",
      "Provide user you information like name ,email, phone, years of experince desired_position, location each in new line\n",
      "Collect techs tack started\n",
      "generete tech questions started\n",
      "['Python', 'java']\n",
      "[\"How would you implement a thread-safe queue in Python\", \"Write a Java program to find the middle element of a singly linked list\", \"Explain how you would optimize the performance of a Python function that is making multiple API calls to an external service\"]\n",
      "these are your questions [\"How would you implement a thread-safe queue in Python\", \"Write a Java program to find the middle element of a singly linked list\", \"Explain how you would optimize the performance of a Python function that is making multiple API calls to an external service\"]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "state = {\n",
    "    \"messages\": [HumanMessage(content=\"Hi I am here for interview\")],\n",
    "    \"answers\": [],\n",
    "    # You can omit tech_stack, questions, etc., and they'll be added during execution\n",
    "}\n",
    "\n",
    "res = compiled_workflow.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c5ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---\\nconfig:\\n  flowchart:\\n    curve: linear\\n---\\ngraph TD;\\n\\t__start__(<p>__start__</p>)\\n\\tgreet_and_consent(greet_and_consent)\\n\\tcollect_info(collect_info)\\n\\tcollect_tech_stack(collect_tech_stack)\\n\\tgenerate_questions(generate_questions)\\n\\thandle_answers(handle_answers)\\n\\tfallback(fallback)\\n\\tend(end)\\n\\t__end__(<p>__end__</p>)\\n\\t__start__ --> greet_and_consent;\\n\\tcollect_info --> collect_tech_stack;\\n\\tcollect_tech_stack --> generate_questions;\\n\\tgenerate_questions --> handle_answers;\\n\\tgreet_and_consent --> collect_info;\\n\\thandle_answers --> end;\\n\\tend --> __end__;\\n\\tclassDef default fill:#f2f0ff,line-height:1.2\\n\\tclassDef first fill-opacity:0\\n\\tclassDef last fill:#bfb6fc\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7de261",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidUpdateError",
     "evalue": "Expected dict, got ('greet_and_consent', {'message': PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"Welcome to TalentScout! I am your Hiring Assistant. Before we proceed, please review our privacy policy and consent to data collection. Type 'I consent' to continue.\")})\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_workflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHi i am here for interview\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages\\langgraph\\pregel\\__init__.py:2719\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2716\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2717\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 2719\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   2720\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2721\u001b[0m     config,\n\u001b[0;32m   2722\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[0;32m   2723\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   2724\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   2725\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   2726\u001b[0m     checkpoint_during\u001b[38;5;241m=\u001b[39mcheckpoint_during,\n\u001b[0;32m   2727\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2729\u001b[0m ):\n\u001b[0;32m   2730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2731\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2732\u001b[0m             \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   2733\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m (ints \u001b[38;5;241m:=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mget(INTERRUPT)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2734\u001b[0m         ):\n",
      "File \u001b[1;32mc:\\Users\\Dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages\\langgraph\\pregel\\__init__.py:2436\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[0;32m   2434\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[0;32m   2435\u001b[0m             loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 2436\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   2437\u001b[0m             [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[0;32m   2438\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   2439\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   2440\u001b[0m             schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[0;32m   2441\u001b[0m         ):\n\u001b[0;32m   2442\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   2443\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[0;32m   2444\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages\\langgraph\\pregel\\runner.py:161\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[0;32m    159\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\Dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\Dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages\\langgraph\\utils\\runnable.py:625\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    624\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 625\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 377\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\Dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages\\langgraph\\pregel\\write.py:99\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     writes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     92\u001b[0m         ChannelWriteEntry(write\u001b[38;5;241m.\u001b[39mchannel, \u001b[38;5;28minput\u001b[39m, write\u001b[38;5;241m.\u001b[39mskip_none, write\u001b[38;5;241m.\u001b[39mmapper)\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[0;32m     98\u001b[0m     ]\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages\\langgraph\\pregel\\write.py:142\u001b[0m, in \u001b[0;36mChannelWrite.do_write\u001b[1;34m(config, writes, allow_passthrough, require_at_least_one_of)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# if we want to persist writes found before hitting a ParentCommand\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# can move this to a finally block\u001b[39;00m\n\u001b[0;32m    141\u001b[0m write: TYPE_SEND \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_SEND]\n\u001b[1;32m--> 142\u001b[0m write(\u001b[43m_assemble_writes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages\\langgraph\\pregel\\write.py:199\u001b[0m, in \u001b[0;36m_assemble_writes\u001b[1;34m(writes)\u001b[0m\n\u001b[0;32m    197\u001b[0m     tuples\u001b[38;5;241m.\u001b[39mappend((TASKS, w))\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteTupleEntry):\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ww \u001b[38;5;241m:=\u001b[39m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    200\u001b[0m         tuples\u001b[38;5;241m.\u001b[39mextend(ww)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteEntry):\n",
      "File \u001b[1;32mc:\\Users\\Dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages\\langgraph\\graph\\state.py:770\u001b[0m, in \u001b[0;36mCompiledStateGraph.attach_node.<locals>._get_updates\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    766\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[0;32m    767\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected dict, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    768\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mINVALID_GRAPH_NODE_RETURN_VALUE,\n\u001b[0;32m    769\u001b[0m     )\n\u001b[1;32m--> 770\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(msg)\n",
      "\u001b[1;31mInvalidUpdateError\u001b[0m: Expected dict, got ('greet_and_consent', {'message': PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"Welcome to TalentScout! I am your Hiring Assistant. Before we proceed, please review our privacy policy and consent to data collection. Type 'I consent' to continue.\")})\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE"
     ]
    }
   ],
   "source": [
    "res = compiled_workflow.invoke({\"messages\":\"Hi i am here for interview\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148a073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Demo Interview ===\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CompiledStateGraph' object has no attribute 'get_node'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Demo Interview ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_input \u001b[38;5;129;01min\u001b[39;00m user_inputs:\n\u001b[1;32m---> 18\u001b[0m     node_func \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_workflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_node\u001b[49m(current_node)\n\u001b[0;32m     19\u001b[0m     next_node, response \u001b[38;5;241m=\u001b[39m node_func(test_state, user_input)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYou: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CompiledStateGraph' object has no attribute 'get_node'"
     ]
    }
   ],
   "source": [
    "# Test the graph workflow with a demo interview\n",
    "\n",
    "test_state = {}\n",
    "current_node = \"greet_and_consent\"\n",
    "\n",
    "# Simulated user inputs for each step\n",
    "user_inputs = [\n",
    "    \"I consent\",  # Consent\n",
    "    \"John Doe\\njohn@example.com\\n1234567890\\n5\\nSoftware Engineer\\nNew York\",  # Candidate info\n",
    "    \"Python, Django, PostgreSQL\",  # Tech stack\n",
    "    \"Answer to Q1\",  # Tech Q1\n",
    "    \"Answer to Q2\",  # Tech Q2\n",
    "    \"Answer to Q3\",  # Tech Q3\n",
    "]\n",
    "\n",
    "print(\"=== Demo Interview ===\")\n",
    "for user_input in user_inputs:\n",
    "    node_func = compiled_workflow.get_node(current_node)\n",
    "    next_node, response = node_func(test_state, user_input)\n",
    "    print(f\"\\nYou: {user_input}\")\n",
    "    if isinstance(response, dict):\n",
    "        message = response.get(\"message\", \"\")\n",
    "        print(f\"Bot: {message}\")\n",
    "        test_state.update(response)\n",
    "    else:\n",
    "        print(f\"Bot: {response}\")\n",
    "    current_node = next_node\n",
    "    if current_node == \"end\":\n",
    "        # Final message\n",
    "        node_func = compiled_workflow.get_node(current_node)\n",
    "        _, response = node_func(test_state, \"\")\n",
    "        if isinstance(response, dict):\n",
    "            print(f\"Bot: {response.get('message', '')}\")\n",
    "        else:\n",
    "            print(f\"Bot: {response}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bfe5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ðŸ§ª Demo Interview ===\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Pregel.stream() got an unexpected keyword argument 'user_input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== ðŸ§ª Demo Interview ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_input \u001b[38;5;129;01min\u001b[39;00m user_inputs:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Use invoke instead of get_node\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     next_node, response \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_workflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ§‘ You: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Display bot's message\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages\\langgraph\\pregel\\__init__.py:2719\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2716\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2717\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 2719\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   2720\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2721\u001b[0m     config,\n\u001b[0;32m   2722\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[0;32m   2723\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   2724\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   2725\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   2726\u001b[0m     checkpoint_during\u001b[38;5;241m=\u001b[39mcheckpoint_during,\n\u001b[0;32m   2727\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2729\u001b[0m ):\n\u001b[0;32m   2730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2731\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2732\u001b[0m             \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   2733\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m (ints \u001b[38;5;241m:=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mget(INTERRUPT)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2734\u001b[0m         ):\n",
      "\u001b[1;31mTypeError\u001b[0m: Pregel.stream() got an unexpected keyword argument 'user_input'"
     ]
    }
   ],
   "source": [
    "# Test the graph workflow with a demo interview\n",
    "\n",
    "test_state = {}\n",
    "current_node = \"greet_and_consent\"\n",
    "\n",
    "# Simulated user inputs for each step\n",
    "user_inputs = [\n",
    "    \"I consent\",  # Consent\n",
    "    \"John Doe\\njohn@example.com\\n1234567890\\n5\\nSoftware Engineer\\nNew York\",  # Candidate info\n",
    "    \"Python, Django, PostgreSQL\",  # Tech stack\n",
    "    \"Answer to Q1\",  # Tech Q1\n",
    "    \"Answer to Q2\",  # Tech Q2\n",
    "    \"Answer to Q3\",  # Tech Q3\n",
    "]\n",
    "\n",
    "print(\"=== ðŸ§ª Demo Interview ===\")\n",
    "for user_input in user_inputs:\n",
    "    # Use invoke instead of get_node\n",
    "    next_node, response = compiled_workflow.invoke(current_node, test_state, user_input=user_input)\n",
    "\n",
    "    print(f\"\\nðŸ§‘ You: {user_input}\")\n",
    "    \n",
    "    # Display bot's message\n",
    "    if isinstance(response, dict):\n",
    "        message = response.get(\"message\", \"\")\n",
    "        print(f\"ðŸ¤– Bot: {message}\")\n",
    "        test_state.update(response)\n",
    "    else:\n",
    "        print(f\"ðŸ¤– Bot: {response}\")\n",
    "    \n",
    "    current_node = next_node\n",
    "    \n",
    "    if current_node == \"end\":\n",
    "        # Final message\n",
    "        _, final_response = compiled_workflow.invoke(current_node, test_state)\n",
    "        if isinstance(final_response, dict):\n",
    "            print(f\"ðŸ¤– Bot: {final_response.get('message', '')}\")\n",
    "        else:\n",
    "            print(f\"ðŸ¤– Bot: {final_response}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0311f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb41f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 09:56:29.760 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.932 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\Dikshit\\miniconda3\\envs\\hiring-agent\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-06-08 09:56:29.933 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.933 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.933 Session state does not function when running a script without `streamlit run`\n",
      "2025-06-08 09:56:29.934 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.935 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.935 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.936 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.936 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.936 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.937 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.938 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.939 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.943 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.944 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.945 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.945 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.946 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.946 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.948 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.948 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-08 09:56:29.949 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "# from .chatbot_graph import workflow\n",
    "\n",
    "st.title(\"TalentScout Hiring Assistant\")\n",
    "\n",
    "if \"state\" not in st.session_state:\n",
    "    st.session_state.state = {}\n",
    "    st.session_state.node = \"greet_and_consent\"\n",
    "    st.session_state.history = []\n",
    "\n",
    "user_input = st.text_input(\"You:\", key=\"user_input\")\n",
    "\n",
    "if st.button(\"Send\") or user_input:\n",
    "    # Use compiled workflow to get the next node and response\n",
    "    node_func = compiled_workflow.get_node(st.session_state.node)\n",
    "    next_node, response = node_func(st.session_state.state, user_input)\n",
    "    st.session_state.node = next_node\n",
    "    if isinstance(response, dict):\n",
    "        st.session_state.state.update(response)\n",
    "        st.session_state.history.append((\"Bot\", response.get(\"message\", \"\")))\n",
    "    else:\n",
    "        st.session_state.history.append((\"Bot\", str(response)))\n",
    "    st.session_state.history.append((\"You\", user_input))\n",
    "    st.session_state.user_input = \"\"  # Clear input\n",
    "\n",
    "for speaker, message in st.session_state.history:\n",
    "    st.markdown(f\"**{speaker}:** {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e693b8db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiring-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
